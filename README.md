# BanditAlgorithms

A collection of simple and modular implementations for solving the stochastic multi-armed bandit problem.

## Overview

This repository includes classic algorithms for K-armed bandits, implemented with clarity and educational value in mind. The goal is to offer a clean and accessible framework for experimenting with online decision-making strategies.

Included algorithms:
- **Epsilon-Greedy**
- **Upper Confidence Bound (UCB1, UCB2)**
- **MOSS (Minimax Optimal Strategy in the Stochastic case)**
- **ETC (Explore-Then-Commit)**
- **RLPE (Regularized Least Posterior Estimation)**
- **Softmax Exploration**

These algorithms are evaluated on simple environments with Gaussian or Bernoulli rewards, and their performance is compared in terms of cumulative regret.

## ğŸ› ï¸ Project Structure

```
algos/                  # Bandit algorithms
â”‚
â”œâ”€â”€ UCB/                # UCB-based algorithms (UCB1, UCB2, MOSS, etc.)
â”‚   â”œâ”€â”€ MOSS.py
â”‚   â”œâ”€â”€ UCB1.py
â”‚   â”œâ”€â”€ UCB2.py
â”‚   â””â”€â”€ ucb.py          # Base or helper functions
â”‚
â”œâ”€â”€ ETC.py              # Explore-Then-Commit
â”œâ”€â”€ RLPE.py             # Regularized Least Posterior Estimation
â””â”€â”€ epsilon_greedy.py   # Epsilon-Greedy algorithm

environments/           # Bandit environments
â”œâ”€â”€ Bandit.py           # Base bandit class
â”œâ”€â”€ BernoulliBandit.py  # Bernoulli reward model
â””â”€â”€ GaussianBandit.py   # Gaussian reward model
```

Each algorithm can be plugged into different environments for comparative testing.

## Example Usage

To run a basic comparison between algorithms:

```bash
python prova.py
```

Example output includes plots like cumulative regret over time.

## ğŸ”§ Requirements

- Python 3.8+
- NumPy
- Matplotlib
